# NORD/accounting/views.py
from __future__ import annotations
from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import viewsets, status, response
from rest_framework.decorators import action, api_view
from django.db.models import Q, Sum, Count, F, Value as V
from django.db.models.functions import TruncDate, Coalesce, Cast
from django.db import transaction
from django.utils import timezone
from datetime import timedelta
from itertools import product, combinations
from multitenancy.api_utils import generic_bulk_create, generic_bulk_update, generic_bulk_delete
from multitenancy.models import CustomUser, Company, Entity
from multitenancy.mixins import ScopedQuerysetMixin
from .models import (Currency, Account, Transaction, JournalEntry, Rule, CostCenter, Bank, BankAccount, BankTransaction, Reconciliation, CostCenter,ReconciliationTask, ReconciliationConfig)
from .serializers import (CurrencySerializer, AccountSerializer, TransactionSerializer, CostCenterSerializer, JournalEntrySerializer, RuleSerializer, BankSerializer, BankAccountSerializer, BankTransactionSerializer, ReconciliationSerializer, TransactionListSerializer,ReconciliationTaskSerializer,ReconciliationConfigSerializer)
from .services.transaction_service import *
from .utils import parse_ofx_text, decode_ofx_content, generate_ofx_transaction_hash, find_book_combos
from datetime import datetime
import pandas as pd
from decimal import Decimal
from django.db.models import DecimalField, DateField
from django.db.utils import OperationalError
from itertools import product
from networkx.algorithms import bipartite
import networkx as nx
from openpyxl import Workbook
from openpyxl.utils.dataframe import dataframe_to_rows
from django.http import HttpResponse
from bisect import bisect_left, bisect_right
from .tasks import match_many_to_many_task
#from celery.task.control import inspect
from nord_backend.celery import app
from multitenancy.api_utils import _to_bool
from django_filters.rest_framework import DjangoFilterBackend
from rest_framework import filters as drf_filters
from .filters import BankTransactionFilter, TransactionFilter
from django.db import transaction as db_tx
import uuid
import os
from django.db import transaction as db_tx
from rest_framework.decorators import action
from rest_framework.response import Response
from accounting.models import BankTransaction, JournalEntry, Reconciliation
from accounting.services.bank_structs import ensure_pending_bank_structs, ensure_gl_account_for_bank

import logging

from datetime import date
from django.db.models import Prefetch
from rest_framework.decorators import action
from rest_framework.response import Response
from rest_framework import status

from accounting.models import Reconciliation, BankTransaction, JournalEntry

# accounting/views_embeddings.py

import os
import time
import requests
from typing import List, Dict, Any, Optional

from rest_framework.views import APIView
from rest_framework.permissions import AllowAny
from rest_framework.response import Response
from rest_framework import status

import logging

from celery import current_app
from celery.result import AsyncResult
from django.db.models import Q
from rest_framework.permissions import IsAdminUser
from rest_framework.response import Response
from rest_framework import status
from rest_framework.views import APIView

from .models import Account, BankTransaction, Transaction
from .serializers import (
    StartEmbeddingBackfillSerializer,
    TaskIdSerializer,
    TaskStatusSerializer,
)
from .services.embedding_client import EmbeddingClient, _embed_url
from .tasks import generate_missing_embeddings

from rest_framework.views import APIView
from rest_framework.response import Response
from rest_framework import status
from django.conf import settings

#from .permissions import HasEmbeddingsApiKey  # or AllowAny for testing if you prefer
from .serializers import EmbedTestSerializer, BackfillSerializer, EmbeddingSearchSerializer
from django.core.cache import cache
from core.models import Job
from core.serializers import JobSerializer
from pgvector.django import CosineDistance

from core.chat.retrieval import _vec_stats, _snippet, _strip_accents, json_safe

def _mean_date(dates):
    """Return the average (mean) of a list of date objects, or None."""
    ds = [d for d in dates if d]
    if not ds:
        return None
    avg_ord = sum(d.toordinal() for d in ds) / len(ds)
    # round to nearest day for stability
    return date.fromordinal(int(round(avg_ord)))

log = logging.getLogger("recon")  # or logging.getLogger(__name__)

def _dbg(tag, **k):  # keep logs consistent with the service
    parts = " ".join(f"{kk}={vv}" for kk, vv in k.items())
    log.debug("[%s] %s", tag, parts)


def _info(tag, **k):
    parts = " ".join(f"{kk}={vv}" for kk, vv in k.items())
    log.info("[%s] %s", tag, parts)


def _warn(tag, **k):
    parts = " ".join(f"{kk}={vv}" for kk, vv in k.items())
    log.warning("[%s] %s", tag, parts)

# Currency ViewSet
class CurrencyViewSet(viewsets.ModelViewSet):
    queryset = Currency.objects.all()
    serializer_class = CurrencySerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)

# Account ViewSet
class CostCenterViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = CostCenter.objects.all()
    serializer_class = CostCenterSerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)

# Account ViewSet
class AccountViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = Account.objects.all()
    serializer_class = AccountSerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)

class AccountSummaryView(APIView):
    def get(self, request, *args, **kwargs):
        company_id = request.query_params.get('company_id')
        entity_id = request.query_params.get('entity_id')
        min_depth = int(request.query_params.get('min_depth', 1))
        include_pending = request.query_params.get('include_pending', 'false').lower() == 'true'
        beginning_date = request.query_params.get('beginning_date')
        end_date = request.query_params.get('end_date')

        if not company_id:
            return Response({"error": "Company ID is required"}, status=status.HTTP_400_BAD_REQUEST)

        accounts_with_balances = Account.get_accounts_summary(
            company_id, entity_id, min_depth, include_pending, beginning_date, end_date
        )
        data = [{'account_code': account.account_code, 'balance': balance} for account, balance in accounts_with_balances]
        return Response(data)

# Account ViewSet
class ReconciliationViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = Reconciliation.objects.all()
    serializer_class = ReconciliationSerializer
    
    # Bulk operations
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        #data = request.data
        #bank_filters = data.get("bank_filters", {})
        print(request.data)
        ids = request.data if isinstance(request.data, list) else []
    
        if not ids:
            return Response(
                {'detail': 'Provide a non-empty list of IDs to delete.'},
                status=status.HTTP_400_BAD_REQUEST
            )
        
        return generic_bulk_delete(self, ids)
    
    @action(methods=['get'], detail=False, url_path='summaries')
    def summaries(self, request, *args, **kwargs):
        """
        Return reconciliations in a compact, 'matches-like' format.

        Query params:
          - status: comma-separated list of statuses (default: matched,approved)
        """
        status_param = request.query_params.get("status", "matched,approved")
        wanted_status = [s.strip() for s in status_param.split(",") if s.strip()]

        qs = self.filter_queryset(
            self.get_queryset().filter(status__in=wanted_status)
        )

        # Prefetch to avoid N+1 queries
        bank_qs = BankTransaction.objects.select_related(
            "bank_account", "bank_account__entity", "currency"
        )
        book_qs = JournalEntry.objects.select_related(
            "account", "account__bank_account",
            "transaction", "transaction__entity", "transaction__currency"
        )

        qs = qs.prefetch_related(
            Prefetch("bank_transactions", queryset=bank_qs),
            Prefetch("journal_entries", queryset=book_qs),
        )

        # Pagination support
        page = self.paginate_queryset(qs)
        recs = page if page is not None else qs

        results = []
        for rec in recs:
            banks = list(rec.bank_transactions.all())
            books = list(rec.journal_entries.all())

            bank_ids = [b.id for b in banks]
            book_ids = [j.id for j in books]

            # sums
            bank_sum = sum((b.amount or 0) for b in banks)
            book_sum = 0
            for je in books:
                eff = je.get_effective_amount()
                if eff is not None:
                    book_sum += eff

            # signed difference (bank - book)
            difference = bank_sum - book_sum

            # average dates
            bank_avg = _mean_date([b.date for b in banks])
            # prefer entry.date, fall back to transaction.date
            book_dates = [je.date or (je.transaction.date if je.transaction_id else None) for je in books]
            book_avg = _mean_date(book_dates)

            # descriptions (collapse duplicates; short, readable)
            def _collapse_desc(items, getter, max_unique=5):
                seen = []
                for it in items:
                    txt = getter(it)
                    if txt and txt not in seen:
                        seen.append(txt)
                    if len(seen) >= max_unique:
                        break
                return " | ".join(seen)
            
            def get_bank_transaction_summary(bank_items):
                lines = []
                for tx in bank_items:
                    lines.append(f"ID: {tx.id}, Date: {tx.date}, Amount: {tx.amount}, Desc: {tx.description}")
                return "\n".join(lines)
        
            def get_journal_entries_summary(book_items):
                lines = []
                for entry in book_items:
                    account_code = entry.account.account_code if entry.account_id else "N/A"
                    account_name = entry.account.name if entry.account_id else "N/A"
                    direction = "DEBIT" if entry.debit_amount else "CREDIT"
                    eff = entry.get_effective_amount()
                    desc = entry.transaction.description if entry.transaction_id else ""
                    lines.append(
                        f"ID: {entry.transaction.id if entry.transaction_id else entry.id}, "
                        f"Date: {entry.date}, JE: {direction} {eff} - "
                        f"({account_code}) {account_name}, Desc: {desc}"
                    )
                return "\n".join(lines)
            
            bank_description = get_bank_transaction_summary(#_collapse_desc(
                banks)#, lambda b: b.description)
            book_description = get_journal_entries_summary(#_collapse_desc(
                books#, lambda je: (je.transaction.description if je.transaction_id else None)
            )
            
            
            
            results.append({
                "reconciliation_id": rec.id,
                "bank_ids": bank_ids,
                "book_ids": book_ids,
                "bank_description": bank_description,
                "book_description": book_description,
                "bank_sum_value": float(bank_sum),
                "book_sum_value": float(book_sum),
                "difference": float(difference),
                "bank_avg_date": bank_avg.isoformat() if bank_avg else None,
                "book_avg_date": book_avg.isoformat() if book_avg else None,
                "reference": rec.reference,
                "notes": rec.notes,
            })

        if page is not None:
            return self.get_paginated_response(results)
        return Response(results, status=status.HTTP_200_OK)
    
# Transaction ViewSet
class TransactionViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    #queryset = Transaction.objects.select_related('company').prefetch_related('journal_entries')
    queryset = (
        Transaction.objects
        .select_related('company', 'currency', 'entity')
        .prefetch_related(
            'journal_entries',
            # add more if needed
        )
    )
    
    filter_backends = [DjangoFilterBackend, drf_filters.SearchFilter, drf_filters.OrderingFilter]
    filterset_class = TransactionFilter
    search_fields = ["description", "entity__name", "journal_entries__account__name"]
    ordering_fields = ["date", "amount", "id", "created_at"]
    ordering = ["-date", "-id"]
    
    def get_serializer_class(self):
        if self.action == 'list':
            return TransactionListSerializer
        return TransactionSerializer

    # Bulk operations
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data
        return generic_bulk_delete(self, ids)

    # Post a transaction
    @action(detail=True, methods=['post'])
    def post(self, request, pk=None):
        transaction = self.get_object()
        try:
            post_transaction(transaction)
            return Response({'status': 'Transaction posted successfully'})
        except ValueError as e:
            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)

    # Unpost a transaction
    @action(detail=True, methods=['post'])
    def unpost(self, request, pk=None):
        transaction = self.get_object()
        try:
            unpost_transaction(transaction)
            return Response({'status': 'Transaction unposted successfully'})
        except ValueError as e:
            return Response({'error': str(e)}, status=status.HTTP_400_BAD_REQUEST)

    # Cancel a transaction
    @action(detail=True, methods=['post'])
    def cancel(self, request, pk=None):
        transaction = self.get_object()
        if transaction.state == 'posted' and not request.user.is_superuser:
            return Response({'error': 'Only superusers can cancel posted transactions'}, status=status.HTTP_403_FORBIDDEN)

        cancel_transaction(transaction)
        return Response({'status': 'Transaction canceled successfully'})

    # Automatically create a balancing journal entry
    @action(detail=True, methods=['post'])
    def create_balancing_entry(self, request, pk=None):
        transaction = self.get_object()
        account_id = request.data.get('account_id')
        if not account_id:
            return Response({'error': 'Account ID is required'}, status=status.HTTP_400_BAD_REQUEST)

        try:
            balancing_account = Account.objects.get(pk=account_id, company=transaction.company)
        except Account.DoesNotExist:
            return Response({'error': 'Invalid account ID'}, status=status.HTTP_400_BAD_REQUEST)

        journal_entry = create_balancing_journal_entry(transaction, balancing_account)
        if not journal_entry:
            return Response({'message': 'Transaction is already balanced'})

        return Response({'status': 'Balancing journal entry created', 'entry': JournalEntrySerializer(journal_entry).data})

    # Filter transactions by company, status, and balance
    @action(detail=False, methods=['get'])
    def filtered(self, request, tenant_id=None):
        """
        Filters transactions based on query parameters and tenant_id.
        """
        # Extract query parameters
        #company_id = request.query_params.get('company_id')
        status_filter = request.query_params.get('status')
        min_balance = request.query_params.get('min_balance')
        max_balance = request.query_params.get('max_balance')

        # Base queryset
        queryset = self.get_queryset().filter(company__subdomain=tenant_id)  # Use tenant_id here

        # Apply additional filters
        #if company_id:
        #    queryset = queryset.filter(company_id=company_id)
        if status_filter:
            if status_filter != "Todos" and status_filter != "*" and status_filter != "":
                queryset = queryset.filter(state=status_filter)
        if min_balance is not None:
            queryset = queryset.annotate(balance=Sum('journal_entries__debit_amount') - Sum('journal_entries__credit_amount'))
            queryset = queryset.filter(balance__gte=min_balance)
        if max_balance is not None:
            queryset = queryset.annotate(balance=Sum('journal_entries__debit_amount') - Sum('journal_entries__credit_amount'))
            queryset = queryset.filter(balance__lte=max_balance)

        # Serialize and return results
        serializer = self.get_serializer(queryset, many=True)
        return Response(serializer.data)

    

class JournalEntryViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    #queryset = JournalEntry.objects.all()
    queryset = (
        JournalEntry.objects
        .select_related('company', 'account', 'transaction')
        # If you also need cost_center or any other FK, include it too
    )
    serializer_class = JournalEntrySerializer

    # Bulk operations
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)

    # Get journal entries by transaction
    @action(detail=False, methods=['get'])
    def by_transaction(self, request):
        transaction_id = request.query_params.get('transaction_id')
        if not transaction_id:
            return Response({'error': 'Transaction ID is required'}, status=status.HTTP_400_BAD_REQUEST)

        journal_entries = self.queryset.filter(transaction_id=transaction_id)
        serializer = self.get_serializer(journal_entries, many=True)
        return Response(serializer.data)

    # Filter journal entries by status or company
    @action(detail=False, methods=['get'])
    def filtered(self, request):
        status_filter = request.query_params.get('status', None)
        company_id = request.query_params.get('tenant_id', None)

        filters = Q()
        if status_filter:
            filters &= Q(state=status_filter)
        if company_id:
            filters &= Q(transaction__company_id=company_id)

        journal_entries = self.queryset.filter(filters)
        serializer = self.get_serializer(journal_entries, many=True)
        return Response(serializer.data)

    
# Rule ViewSet
class RuleViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = Rule.objects.all()
    serializer_class = RuleSerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)
    
# Bank ViewSet
class BankViewSet(viewsets.ModelViewSet):
    queryset = Bank.objects.all()
    serializer_class = BankSerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)



# BankAccount ViewSet
class BankAccountViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = BankAccount.objects.all()
    serializer_class = BankAccountSerializer
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)
    
# BankTransaction ViewSet
class BankTransactionViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = (
        BankTransaction.objects
        .select_related("bank_account", "bank_account__entity", "currency")  # add "bank_account__bank" if you render it
        .order_by("-date", "-id")
    )
    serializer_class = BankTransactionSerializer

    filter_backends = [DjangoFilterBackend, drf_filters.SearchFilter, drf_filters.OrderingFilter]
    filterset_class = BankTransactionFilter
    search_fields = [
        "description",
        "reference_number",
        "bank_account__name",
        "bank_account__account_number",
        "bank_account__entity__name",  # <-- fixed
    ]
    ordering_fields = ["date", "amount", "id", "created_at"]
    ordering = ["-date", "-id"]

    # If your mixin needs to know how to scope by entity:
    entity_lookup = "bank_account__entity"  # <-- only if your ScopedQuerysetMixin uses this

    def get_queryset(self):
        qs = super().get_queryset()

        # /entities/<entity_id>/... or ?entity_id=...
        entity_id = self.kwargs.get("entity_id") or self.request.query_params.get("entity_id")
        if entity_id:
            qs = qs.filter(bank_account__entity_id=entity_id)

        bank_account_id = self.request.query_params.get("bank_account")
        if bank_account_id:
            qs = qs.filter(bank_account_id=bank_account_id)

        # accept ?status=... or ?status=PENDING,STARTED
        status_param = self.request.query_params.get("status")
        if status_param:
            if "," in status_param:
                qs = qs.filter(status__in=[s.strip() for s in status_param.split(",") if s.strip()])
            else:
                qs = qs.filter(status=status_param)

        return qs
    
    @action(methods=['post'], detail=False)
    def bulk_create(self, request, *args, **kwargs):
        return generic_bulk_create(self, request.data)

    @action(methods=['put'], detail=False)
    def bulk_update(self, request, *args, **kwargs):
        return generic_bulk_update(self, request.data)

    @action(methods=['delete'], detail=False)
    def bulk_delete(self, request, *args, **kwargs):
        ids = request.data  # Assuming request.data is a list of IDs
        return generic_bulk_delete(self, ids)
    
    @action(detail=False, methods=['post'])
    def import_ofx(self, request, *args, **kwargs):
        """
        Accepts a JSON body like:
        {
          "files": [
            { "name": "foo.OFX", "base64Data": "..." },
            { "ofx_text": "OFXHEADER:100\nDATA:OFXSGML\n..." }
          ]
        }

        For each file:
         - Decode
         - Parse
         - Check references
        Returns a combined structure describing each file's parse and missing references
        """
        files_data = request.data.get("files")
        if not files_data or not isinstance(files_data, list):
            return Response({"error": "Please provide 'files' as a list."},
                            status=status.HTTP_400_BAD_REQUEST)

        import_results = []

        # We'll keep track of overall missing references
        # But different files can have different bank_code or account_id
        for idx, file_item in enumerate(files_data):
            # 1) decode
            ofx_content = decode_ofx_content(file_item)
            if not ofx_content:
                import_results.append({
                    "index": idx,
                    "error": "No valid ofx_text or base64Data found.",
                })
                continue

            # 2) parse
            try:
                parsed = parse_ofx_text(ofx_content)
            except Exception as e:
                import_results.append({
                    "index": idx,
                    "error": f"Failed to parse OFX: {str(e)}",
                })
                continue

            bank_code = int(parsed.get("bank_code"))
            account_id = parsed.get("account_id")
            transactions = parsed.get("transactions", [])
            
            
            
            # 3) check references
            missing_bank = None
            missing_bank_account = None

            bank_obj = Bank.objects.filter(bank_code=bank_code).first()
            if bank_obj:
                bank_data = {
                    "result": "Success",
                    "message": "Bank found.",
                    "value": BankSerializer(bank_obj).data
                }
                bank_num = bank_obj.bank_code
            else:
                bank_data = {
                    "result": "Error",
                    "message": f"Bank code '{bank_code}' not found.",
                    "value": bank_code
                }
                bank_num = bank_code

            bank_acct_obj = BankAccount.objects.filter(bank__bank_code=bank_code, account_number=account_id).first()
            if bank_acct_obj:
                account_data = {
                    "result": "Success",
                    "message": "BankAccount found.",
                    "value": BankAccountSerializer(bank_acct_obj).data
                }
                acct_num = bank_acct_obj.account_number
            else:
                account_data = {
                    "result": "Error",
                    "message": f"BankAccount '{account_id}' not found.",
                    "value": account_id
                }
                acct_num = account_id
            
            for tx in transactions:
                raw_date = tx.get("date")
                parsed_date = None
                if raw_date:
                    try:
                        parsed_date = datetime.strptime(raw_date, "%Y-%m-%d").date()
                    except:
                        pass
                # build the fields
                date_str = parsed_date.isoformat() if parsed_date else ""
                amount_val = tx.get("amount", 0.0)
                transaction_type = tx.get("transaction_type", "")
                memo = tx.get("description", "")
                  # from the DB or your code
                
                # 1) Generate the hash
                tx_hash = generate_ofx_transaction_hash(
                    date_str=date_str,
                    amount=amount_val,
                    transaction_type=transaction_type,
                    memo=memo,
                    bank_number=bank_num,
                    account_number=acct_num
                )
                tx['tx_hash'] = tx_hash
                
                existing = BankTransaction.objects.filter(tx_hash=tx_hash).first()
                if existing:
                    tx['status'] = 'duplicate'
                else:
                    tx['status'] = 'pending'
                    
            # Summarize result for this file
            import_results.append({
                "index": idx,
                "filename": file_item.get("name"),
                "bank": bank_data,
                "account": account_data,
                "transactions": transactions
            })

        return Response({"import_results": import_results}, status=status.HTTP_200_OK)
    
   
    
    @action(detail=False, methods=['get'])
    def unreconciled(self, request, tenant_id):
        unreconciled_transactions = BankTransaction.objects.filter(reconciliations__isnull=True)
        serializer = BankTransactionSerializer(unreconciled_transactions, many=True)
        return response.Response(serializer.data)
    
    @action(detail=False, methods=['post'])
    def finalize_reconciliation_matches(self, request, *args, **kwargs):
        """
        Finalize a batch of reconciliation matches (optionally creating an adjustment on one side).
        """
        request_id = str(uuid.uuid4())
    
        data = request.data
        matches = data.get("matches", [])
        adjustment_side = data.get("adjustment_side", "none")  # "bank" | "journal" | "none"
        reference = data.get("reference", "")
        notes = data.get("notes", "")
    
        _info("finalize_begin",
              request_id=request_id,
              n_matches=len(matches),
              adjustment_side=adjustment_side,
              reference=reference or "",
              notes_len=len(notes or ""))
    
        # CHANGED: richer output container
        created_records = []
        problems = []
    
        with db_tx.atomic():
            for idx, match in enumerate(matches, start=1):
                try:
                    raw_bank = match.get("bank_transaction_ids", [])
                    raw_journal = match.get("journal_entry_ids", [])
                    bank_ids = list({int(x) for x in raw_bank if x is not None})
                    journal_ids = list({int(x) for x in raw_journal if x is not None})
    
                    _dbg("match_recv",
                         request_id=request_id, i=idx,
                         bank_ids=bank_ids, journal_ids=journal_ids)
    
                    if not bank_ids or not journal_ids:
                        problems.append({"reason": "empty_ids", "match": match})
                        _warn("match_skip_empty_ids", request_id=request_id, i=idx)
                        continue
    
                    already = BankTransaction.objects.filter(
                        id__in=bank_ids, reconciliations__status__in=["matched", "approved"]
                    ).exists() or JournalEntry.objects.filter(
                        id__in=journal_ids, reconciliations__status__in=["matched", "approved"]
                    ).exists()
                    _dbg("match_precheck_already",
                         request_id=request_id, i=idx, already_reconciled=already)
    
                    if already:
                        problems.append({"reason": "already_reconciled",
                                         "bank_ids": bank_ids, "journal_ids": journal_ids})
                        _warn("match_skip_already_reconciled", request_id=request_id, i=idx)
                        continue
    
                    sp = db_tx.savepoint()
                    try:
                        bank_qs = (BankTransaction.objects
                                   .filter(id__in=bank_ids)
                                   .select_for_update(of=('self',)))
                        je_qs = (JournalEntry.objects
                                 .filter(id__in=journal_ids)
                                 .select_for_update(of=('self',)))
    
                        bank_txs = list(bank_qs)
                        journal_entries = list(je_qs)
    
                        _dbg("match_locked_counts",
                             request_id=request_id, i=idx,
                             bank_locked=len(bank_txs), journal_locked=len(journal_entries))
    
                        if not bank_txs or not journal_entries:
                            problems.append({"reason": "not_found",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_not_found", request_id=request_id, i=idx)
                            db_tx.savepoint_commit(sp)
                            continue
    
                        company_set = {bt.company_id for bt in bank_txs} | {je.company_id for je in journal_entries}
                        currency_set = {bt.currency_id for bt in bank_txs} | {
                            je.transaction.currency_id for je in journal_entries if je.transaction_id
                        }
    
                        _dbg("match_sets",
                             request_id=request_id, i=idx,
                             companies=list(company_set), currencies=list(currency_set))
    
                        if len(company_set) != 1:
                            problems.append({"reason": "mixed_company",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_mixed_company", request_id=request_id, i=idx, companies=list(company_set))
                            db_tx.savepoint_commit(sp)
                            continue
                        company_id = next(iter(company_set))
    
                        if len(currency_set) != 1:
                            problems.append({"reason": "mixed_currency",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_mixed_currency", request_id=request_id, i=idx, currencies=list(currency_set))
                            db_tx.savepoint_commit(sp)
                            continue
                        currency_id = next(iter(currency_set))
    
                        pending_ba, pending_gl = ensure_pending_bank_structs(company_id, currency_id=currency_id)
                        _dbg("pending_structs",
                             request_id=request_id, i=idx,
                             pending_ba_id=pending_ba.id, pending_gl_id=pending_gl.id)
    
                        sum_bank = sum((bt.amount for bt in bank_txs), Decimal("0"))
                        sum_journal = sum(((je.get_effective_amount() or Decimal("0")) for je in journal_entries),
                                          Decimal("0"))
                        diff = sum_bank - sum_journal
    
                        _dbg("totals_initial",
                             request_id=request_id, i=idx,
                             sum_bank=str(sum_bank), sum_journal=str(sum_journal), diff=str(diff))
    
                        bank_account_set = {bt.bank_account_id for bt in bank_txs if bt.bank_account_id}
                        if adjustment_side != "none" and diff != Decimal("0"):
                            if adjustment_side == "bank":
                                if len(bank_account_set) != 1:
                                    problems.append({
                                        "reason": "cannot_adjust_bank_with_multiple_accounts",
                                        "bank_account_ids": list(bank_account_set),
                                        "bank_ids": bank_ids, "journal_ids": journal_ids,
                                    })
                                    _warn("adjust_skip_multi_ba",
                                          request_id=request_id, i=idx, bank_accounts=list(bank_account_set))
                                else:
                                    adjustment_amount = sum_journal - sum_bank
                                    bt0 = bank_txs[0]
                                    _dbg("adjust_bank_create",
                                         request_id=request_id, i=idx, amount=str(adjustment_amount))
                                    adj_bt = BankTransaction.objects.create(
                                        company_id=company_id,
                                        bank_account=bt0.bank_account,
                                        date=bt0.date,
                                        currency=bt0.currency,
                                        amount=adjustment_amount,
                                        description="Adjustment record for reconciliation",
                                        status="pending",
                                        tx_hash="adjustment_for_rec",
                                    )
                                    bank_txs.append(adj_bt)
                                    sum_bank += adjustment_amount
    
                            elif adjustment_side == "journal":
                                adjustment_amount = sum_bank - sum_journal
                                je0 = journal_entries[0]
                                debit_amount = adjustment_amount if adjustment_amount > 0 else None
                                credit_amount = (-adjustment_amount) if adjustment_amount < 0 else None
                                _dbg("adjust_journal_create",
                                     request_id=request_id, i=idx,
                                     amount=str(adjustment_amount),
                                     debit=str(debit_amount or 0), credit=str(credit_amount or 0))
                                adj_je = JournalEntry.objects.create(
                                    company_id=company_id,
                                    transaction=je0.transaction,
                                    account=(pending_gl if (je0.account is None or
                                                            getattr(je0.account, "bank_account_id", None) is None)
                                             else je0.account),
                                    cost_center=je0.cost_center,
                                    debit_amount=debit_amount,
                                    credit_amount=credit_amount,
                                    state="pending",
                                    date=je0.date or je0.transaction.date,
                                    bank_designation_pending=True,
                                )
                                journal_entries.append(adj_je)
                                sum_journal += adjustment_amount
    
                        final_diff = sum_bank - sum_journal
                        rec_status = "matched" if final_diff == Decimal("0") else "pending"
    
                        _dbg("totals_final",
                             request_id=request_id, i=idx,
                             sum_bank=str(sum_bank), sum_journal=str(sum_journal),
                             final_diff=str(final_diff), rec_status=rec_status)
    
                        bank_ids_used = [x.id for x in bank_txs]
                        journal_ids_used = [x.id for x in journal_entries]
    
                        bank_ids_str = ", ".join(str(x) for x in bank_ids_used)
                        journal_ids_str = ", ".join(str(x) for x in journal_ids_used)
                        combined_notes = (
                            f"{notes}\n"
                            f"Bank IDs: {bank_ids_str}\n"
                            f"Journal IDs: {journal_ids_str}\n"
                            f"Difference: {final_diff}"
                        )
    
                        rec = Reconciliation.objects.create(
                            company_id=company_id,
                            status=rec_status,
                            reference=reference,
                            notes=combined_notes,
                        )
                        rec.bank_transactions.set(bank_txs)
                        rec.journal_entries.set(journal_entries)
    
                        # CHANGED: push rich record to output
                        created_records.append({
                            "reconciliation_id": rec.id,
                            "status": rec_status,
                            "bank_ids_used": bank_ids_used,
                            "journal_ids_used": journal_ids_used,
                        })
    
                        _info("reconciliation_created",
                              request_id=request_id, i=idx,
                              reconciliation_id=rec.id, status=rec_status,
                              n_bank=len(bank_txs), n_journal=len(journal_entries))
    
                        if len(bank_account_set) == 1:
                            target_ba_id = next(iter(bank_account_set))
                            target_ba = next(bt.bank_account for bt in bank_txs if bt.bank_account_id == target_ba_id)
                            target_gl = ensure_gl_account_for_bank(company_id, target_ba)
    
                            changed = 0
                            for je in journal_entries:
                                acct_ba_id = getattr(je.account, "bank_account_id", None) if je.account_id else None
                                if (acct_ba_id == pending_ba.id or je.account_id is None or
                                        getattr(je, "bank_designation_pending", False)):
                                    fields = []
                                    if je.account_id != target_gl.id:
                                        je.account_id = target_gl.id
                                        fields.append("account")
                                    if hasattr(je, "bank_designation_pending") and je.bank_designation_pending:
                                        je.bank_designation_pending = False
                                        fields.append("bank_designation_pending")
                                    if fields:
                                        je.save(update_fields=fields)
                                        changed += 1
                            _dbg("promotion_done",
                                 request_id=request_id, i=idx, reassigned_lines=changed, target_gl_id=target_gl.id)
                        else:
                            problems.append({
                                "reason": "multiple_bank_accounts_in_match",
                                "bank_account_ids": list(bank_account_set),
                                "bank_ids": bank_ids, "journal_ids": journal_ids,
                            })
                            _warn("promotion_skipped_multi_ba",
                                  request_id=request_id, i=idx, bank_accounts=list(bank_account_set))
    
                        db_tx.savepoint_commit(sp)
    
                    except Exception as e:
                        db_tx.savepoint_rollback(sp)
                        problems.append({"reason": "exception", "error": str(e),
                                         "bank_ids": bank_ids, "journal_ids": journal_ids})
                        _warn("match_exception", request_id=request_id, i=idx, error=str(e))
    
                except Exception as outer_e:
                    problems.append({"reason": "outer_exception", "error": str(outer_e),
                                     "match": match})
                    _warn("match_outer_exception", request_id=request_id, i=idx, error=str(outer_e))
    
        _info("finalize_end",
              request_id=request_id,
              created=len(created_records),
              problems=len(problems),
              created_records=created_records)
    
        # CHANGED: richer return
        return Response({"created": created_records, "problems": problems})
    '''
    @action(detail=False, methods=['post'])
    def finalize_reconciliation_matches(self, request, *args, **kwargs):
        """
        Finalize a batch of reconciliation matches (optionally creating an adjustment on one side).
        """
        request_id = str(uuid.uuid4())

        data = request.data
        matches = data.get("matches", [])
        adjustment_side = data.get("adjustment_side", "none")  # "bank" | "journal" | "none"
        reference = data.get("reference", "")
        notes = data.get("notes", "")

        _info("finalize_begin",
              request_id=request_id,
              n_matches=len(matches),
              adjustment_side=adjustment_side,
              reference=reference or "",
              notes_len=len(notes or ""))

        created_ids = []
        problems = []

        with db_tx.atomic():
            for idx, match in enumerate(matches, start=1):
                try:
                    raw_bank = match.get("bank_transaction_ids", [])
                    raw_journal = match.get("journal_entry_ids", [])
                    bank_ids = list({int(x) for x in raw_bank if x is not None})
                    journal_ids = list({int(x) for x in raw_journal if x is not None})

                    _dbg("match_recv",
                         request_id=request_id, i=idx,
                         bank_ids=bank_ids, journal_ids=journal_ids)

                    if not bank_ids or not journal_ids:
                        problems.append({"reason": "empty_ids", "match": match})
                        _warn("match_skip_empty_ids", request_id=request_id, i=idx)
                        continue

                    # Guard: already reconciled as matched/approved
                    already = BankTransaction.objects.filter(
                        id__in=bank_ids, reconciliations__status__in=["matched", "approved"]
                    ).exists() or JournalEntry.objects.filter(
                        id__in=journal_ids, reconciliations__status__in=["matched", "approved"]
                    ).exists()
                    _dbg("match_precheck_already",
                         request_id=request_id, i=idx, already_reconciled=already)

                    if already:
                        problems.append({"reason": "already_reconciled",
                                         "bank_ids": bank_ids, "journal_ids": journal_ids})
                        _warn("match_skip_already_reconciled", request_id=request_id, i=idx)
                        continue

                    sp = db_tx.savepoint()
                    try:
                        bank_qs = (BankTransaction.objects
                                   .select_for_update()
                                   #.select_related("bank_account", "currency")
                                   .filter(id__in=bank_ids).select_for_update(of=('self',)))
                        je_qs = (JournalEntry.objects
                                 .select_for_update()
                                 #.select_related("account", "account__bank_account",
                                 #                "transaction", "transaction__currency")
                                 .filter(id__in=journal_ids).select_for_update(of=('self',)))

                        bank_txs = list(bank_qs)
                        journal_entries = list(je_qs)

                        _dbg("match_locked_counts",
                             request_id=request_id, i=idx,
                             bank_locked=len(bank_txs), journal_locked=len(journal_entries))

                        if not bank_txs or not journal_entries:
                            problems.append({"reason": "not_found",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_not_found", request_id=request_id, i=idx)
                            db_tx.savepoint_commit(sp)
                            continue

                        company_set = {bt.company_id for bt in bank_txs} | {je.company_id for je in journal_entries}
                        currency_set = {bt.currency_id for bt in bank_txs} | {
                            je.transaction.currency_id for je in journal_entries if je.transaction_id
                        }

                        _dbg("match_sets",
                             request_id=request_id, i=idx,
                             companies=list(company_set), currencies=list(currency_set))

                        if len(company_set) != 1:
                            problems.append({"reason": "mixed_company",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_mixed_company", request_id=request_id, i=idx, companies=list(company_set))
                            db_tx.savepoint_commit(sp)
                            continue
                        company_id = next(iter(company_set))

                        if len(currency_set) != 1:
                            problems.append({"reason": "mixed_currency",
                                             "bank_ids": bank_ids, "journal_ids": journal_ids})
                            _warn("match_skip_mixed_currency", request_id=request_id, i=idx, currencies=list(currency_set))
                            db_tx.savepoint_commit(sp)
                            continue
                        currency_id = next(iter(currency_set))

                        # Ensure pending structures
                        pending_ba, pending_gl = ensure_pending_bank_structs(company_id, currency_id=currency_id)
                        _dbg("pending_structs",
                             request_id=request_id, i=idx,
                             pending_ba_id=pending_ba.id, pending_gl_id=pending_gl.id)

                        # Totals
                        sum_bank = sum((bt.amount for bt in bank_txs), Decimal("0"))
                        sum_journal = sum(((je.get_effective_amount() or Decimal("0")) for je in journal_entries),
                                          Decimal("0"))
                        diff = sum_bank - sum_journal

                        _dbg("totals_initial",
                             request_id=request_id, i=idx,
                             sum_bank=str(sum_bank), sum_journal=str(sum_journal), diff=str(diff))

                        # Adjustment
                        bank_account_set = {bt.bank_account_id for bt in bank_txs if bt.bank_account_id}
                        if adjustment_side != "none" and diff != Decimal("0"):
                            if adjustment_side == "bank":
                                if len(bank_account_set) != 1:
                                    problems.append({
                                        "reason": "cannot_adjust_bank_with_multiple_accounts",
                                        "bank_account_ids": list(bank_account_set),
                                        "bank_ids": bank_ids, "journal_ids": journal_ids,
                                    })
                                    _warn("adjust_skip_multi_ba",
                                          request_id=request_id, i=idx, bank_accounts=list(bank_account_set))
                                else:
                                    adjustment_amount = sum_journal - sum_bank
                                    bt0 = bank_txs[0]
                                    _dbg("adjust_bank_create",
                                         request_id=request_id, i=idx, amount=str(adjustment_amount))
                                    adj_bt = BankTransaction.objects.create(
                                        company_id=company_id,
                                        bank_account=bt0.bank_account,
                                        date=bt0.date,
                                        currency=bt0.currency,
                                        amount=adjustment_amount,
                                        description="Adjustment record for reconciliation",
                                        status="pending",
                                        tx_hash="adjustment_for_rec",
                                    )
                                    bank_txs.append(adj_bt)
                                    sum_bank += adjustment_amount

                            elif adjustment_side == "journal":
                                adjustment_amount = sum_bank - sum_journal
                                je0 = journal_entries[0]
                                debit_amount = adjustment_amount if adjustment_amount > 0 else None
                                credit_amount = (-adjustment_amount) if adjustment_amount < 0 else None
                                _dbg("adjust_journal_create",
                                     request_id=request_id, i=idx,
                                     amount=str(adjustment_amount),
                                     debit=str(debit_amount or 0), credit=str(credit_amount or 0))
                                adj_je = JournalEntry.objects.create(
                                    company_id=company_id,
                                    transaction=je0.transaction,
                                    account=(pending_gl if (je0.account is None or
                                                            getattr(je0.account, "bank_account_id", None) is None)
                                             else je0.account),
                                    cost_center=je0.cost_center,
                                    debit_amount=debit_amount,
                                    credit_amount=credit_amount,
                                    state="pending",
                                    date=je0.date or je0.transaction.date,
                                    bank_designation_pending=True,
                                )
                                journal_entries.append(adj_je)
                                sum_journal += adjustment_amount

                        final_diff = sum_bank - sum_journal
                        rec_status = "matched" if final_diff == Decimal("0") else "pending"

                        _dbg("totals_final",
                             request_id=request_id, i=idx,
                             sum_bank=str(sum_bank), sum_journal=str(sum_journal),
                             final_diff=str(final_diff), rec_status=rec_status)

                        # Create reconciliation
                        bank_ids_str = ", ".join(str(x.id) for x in bank_txs)
                        journal_ids_str = ", ".join(str(x.id) for x in journal_entries)
                        combined_notes = (
                            f"{notes}\n"
                            f"Bank IDs: {bank_ids_str}\n"
                            f"Journal IDs: {journal_ids_str}\n"
                            f"Difference: {final_diff}"
                        )

                        rec = Reconciliation.objects.create(
                            company_id=company_id,
                            status=rec_status,
                            reference=reference,
                            notes=combined_notes,
                        )
                        rec.bank_transactions.set(bank_txs)
                        rec.journal_entries.set(journal_entries)
                        created_ids.append(rec.id)

                        _info("reconciliation_created",
                              request_id=request_id, i=idx,
                              reconciliation_id=rec.id, status=rec_status,
                              n_bank=len(bank_txs), n_journal=len(journal_entries))

                        # Promotion to specific bank GL (if single bank account)
                        if len(bank_account_set) == 1:
                            target_ba_id = next(iter(bank_account_set))
                            target_ba = next(bt.bank_account for bt in bank_txs if bt.bank_account_id == target_ba_id)
                            target_gl = ensure_gl_account_for_bank(company_id, target_ba)

                            changed = 0
                            for je in journal_entries:
                                acct_ba_id = getattr(je.account, "bank_account_id", None) if je.account_id else None
                                if (acct_ba_id == pending_ba.id or je.account_id is None or
                                        getattr(je, "bank_designation_pending", False)):
                                    fields = []
                                    if je.account_id != target_gl.id:
                                        je.account_id = target_gl.id
                                        fields.append("account")
                                    if hasattr(je, "bank_designation_pending") and je.bank_designation_pending:
                                        je.bank_designation_pending = False
                                        fields.append("bank_designation_pending")
                                    if fields:
                                        je.save(update_fields=fields)
                                        changed += 1
                            _dbg("promotion_done",
                                 request_id=request_id, i=idx, reassigned_lines=changed, target_gl_id=target_gl.id)
                        else:
                            problems.append({
                                "reason": "multiple_bank_accounts_in_match",
                                "bank_account_ids": list(bank_account_set),
                                "bank_ids": bank_ids, "journal_ids": journal_ids,
                            })
                            _warn("promotion_skipped_multi_ba",
                                  request_id=request_id, i=idx, bank_accounts=list(bank_account_set))

                        db_tx.savepoint_commit(sp)

                    except Exception as e:
                        db_tx.savepoint_rollback(sp)
                        problems.append({"reason": "exception", "error": str(e),
                                         "bank_ids": bank_ids, "journal_ids": journal_ids})
                        _warn("match_exception", request_id=request_id, i=idx, error=str(e))
                        # continue to next item

                except Exception as outer_e:
                    problems.append({"reason": "outer_exception", "error": str(outer_e),
                                     "match": match})
                    _warn("match_outer_exception", request_id=request_id, i=idx, error=str(outer_e))
                    # keep looping other matches

        _info("finalize_end",
              request_id=request_id,
              created=len(created_ids),
              problems=len(problems),
              created_ids=created_ids)

        return Response({"reconciliation_ids": created_ids, "problems": problems})
    
    
    @action(detail=False, methods=['post'])
    def finalize_reconciliation_matches(self, request, *args, **kwargs):
        """
        Finalize a batch of reconciliation matches (optionally creating an adjustment on one side).
        """
        request_id = str(uuid.uuid4())
        
        data = request.data
        matches = data.get("matches", [])
        adjustment_side = data.get("adjustment_side", "none")  # "bank" | "journal" | "none"
        reference = data.get("reference", "")
        notes = data.get("notes", "")
        
        _info("finalize_begin",
              request_id=request_id,
              n_matches=len(matches),
              adjustment_side=adjustment_side,
              reference=reference or "",
              notes_len=len(notes or ""))
        
        created_ids = []
        problems = []
    
        with db_tx.atomic():
            for match in matches:
                bank_ids = list({int(x) for x in match.get("bank_transaction_ids", []) if x is not None})
                journal_ids = list({int(x) for x in match.get("journal_entry_ids", []) if x is not None})
                if not bank_ids or not journal_ids:
                    problems.append({"reason": "empty_ids", "match": match})
                    continue
    
                # OPTIONAL: guard against re-using rows already matched/approved
                if BankTransaction.objects.filter(
                    id__in=bank_ids, reconciliations__status__in=["matched", "approved"]
                ).exists() or JournalEntry.objects.filter(
                    id__in=journal_ids, reconciliations__status__in=["matched", "approved"]
                ).exists():
                    problems.append({"reason": "already_reconciled", "bank_ids": bank_ids, "journal_ids": journal_ids})
                    continue
    
                sp = db_tx.savepoint()
                try:
                    # Lock rows
                    bank_qs = (BankTransaction.objects
                               .select_for_update()
                               .select_related("bank_account", "currency")
                               .filter(id__in=bank_ids))
                    je_qs = (JournalEntry.objects
                             .select_for_update()
                             .select_related("account", "account__bank_account", "transaction", "transaction__currency")
                             .filter(id__in=journal_ids))
    
                    bank_txs = list(bank_qs)
                    journal_entries = list(je_qs)
                    if not bank_txs or not journal_entries:
                        problems.append({"reason": "not_found", "bank_ids": bank_ids, "journal_ids": journal_ids})
                        db_tx.savepoint_commit(sp)
                        continue
    
                    # Company & currency consistency
                    company_set = {bt.company_id for bt in bank_txs} | {je.company_id for je in journal_entries}
                    if len(company_set) != 1:
                        problems.append({"reason": "mixed_company", "bank_ids": bank_ids, "journal_ids": journal_ids})
                        db_tx.savepoint_commit(sp)
                        continue
                    company_id = next(iter(company_set))
    
                    currency_set = {bt.currency_id for bt in bank_txs} | {
                        je.transaction.currency_id for je in journal_entries if je.transaction_id
                    }
                    if len(currency_set) != 1:
                        problems.append({"reason": "mixed_currency", "bank_ids": bank_ids, "journal_ids": journal_ids})
                        db_tx.savepoint_commit(sp)
                        continue
                    currency_id = next(iter(currency_set))
    
                    # Ensure pending scaffolding exists
                    pending_ba, pending_gl = ensure_pending_bank_structs(company_id, currency_id=currency_id)
    
                    # Totals
                    sum_bank = sum((bt.amount for bt in bank_txs), Decimal("0"))
                    sum_journal = sum(((je.get_effective_amount() or Decimal("0")) for je in journal_entries), Decimal("0"))
                    diff = sum_bank - sum_journal
    
                    # If match spans multiple bank accounts, we will NOT create bank-side adjustments.
                    bank_account_set = {bt.bank_account_id for bt in bank_txs if bt.bank_account_id}
    
                    # Adjustment (optional)
                    if adjustment_side != "none" and diff != Decimal("0"):
                        if adjustment_side == "bank":
                            if len(bank_account_set) != 1:
                                problems.append({
                                    "reason": "cannot_adjust_bank_with_multiple_accounts",
                                    "bank_account_ids": list(bank_account_set),
                                    "bank_ids": bank_ids, "journal_ids": journal_ids,
                                })
                            else:
                                # Make bank total equal journal total
                                adjustment_amount = sum_journal - sum_bank
                                bt0 = bank_txs[0]
                                adj_bt = BankTransaction.objects.create(
                                    company_id=company_id,
                                    bank_account=bt0.bank_account,
                                    date=bt0.date,
                                    currency=bt0.currency,
                                    amount=adjustment_amount,
                                    description="Adjustment record for reconciliation",
                                    status="pending",
                                    tx_hash="adjustment_for_rec",
                                )
                                bank_txs.append(adj_bt)
                                sum_bank += adjustment_amount
    
                        elif adjustment_side == "journal":
                            # Make journal total equal bank total
                            adjustment_amount = sum_bank - sum_journal
                            je0 = journal_entries[0]
                            debit_amount = adjustment_amount if adjustment_amount > 0 else None
                            credit_amount = (-adjustment_amount) if adjustment_amount < 0 else None
                            adj_je = JournalEntry.objects.create(
                                company_id=company_id,
                                transaction=je0.transaction,
                                account=pending_gl if (je0.account is None or getattr(je0.account, "bank_account_id", None) is None) else je0.account,
                                cost_center=je0.cost_center,
                                debit_amount=debit_amount,
                                credit_amount=credit_amount,
                                state="pending",
                                date=je0.date or je0.transaction.date,
                                bank_designation_pending=True,  # ensure later promotion
                            )
                            journal_entries.append(adj_je)
                            sum_journal += adjustment_amount
    
                    # Final status
                    final_diff = sum_bank - sum_journal
                    rec_status = "matched" if final_diff == Decimal("0") else "pending"
    
                    # Create Reconciliation
                    bank_ids_str = ", ".join(str(x.id) for x in bank_txs)
                    journal_ids_str = ", ".join(str(x.id) for x in journal_entries)
                    combined_notes = f"{notes}\nBank IDs: {bank_ids_str}\nJournal IDs: {journal_ids_str}\nDifference: {final_diff}"
    
                    rec = Reconciliation.objects.create(
                        company_id=company_id,
                        status=rec_status,
                        reference=reference,
                        notes=combined_notes,
                    )
                    rec.bank_transactions.set(bank_txs)
                    rec.journal_entries.set(journal_entries)
                    created_ids.append(rec.id)
    
                    # Promote pending JEs to the specific bank GL (only if a single bank_account is involved)
                    if len(bank_account_set) == 1:
                        target_ba_id = next(iter(bank_account_set))
                        target_ba = next(bt.bank_account for bt in bank_txs if bt.bank_account_id == target_ba_id)
                        target_gl = ensure_gl_account_for_bank(company_id, target_ba)
    
                        for je in journal_entries:
                            acct_ba_id = getattr(je.account, "bank_account_id", None) if je.account_id else None
                            if acct_ba_id == pending_ba.id or je.account_id is None or getattr(je, "bank_designation_pending", False):
                                updates = {}
                                if je.account_id != target_gl.id:
                                    je.account_id = target_gl.id
                                    updates["account"] = target_gl.id
                                if hasattr(je, "bank_designation_pending") and je.bank_designation_pending:
                                    je.bank_designation_pending = False
                                    updates["bank_designation_pending"] = False
                                if updates:
                                    je.save(update_fields=list(updates.keys()))
                    else:
                        problems.append({
                            "reason": "multiple_bank_accounts_in_match",
                            "bank_account_ids": list(bank_account_set),
                            "bank_ids": bank_ids,
                            "journal_ids": journal_ids,
                        })
    
                    db_tx.savepoint_commit(sp)
    
                except Exception as e:
                    db_tx.savepoint_rollback(sp)
                    problems.append({"reason": "exception", "error": str(e), "bank_ids": bank_ids, "journal_ids": journal_ids})
    
        return Response({"reconciliation_ids": created_ids, "problems": problems})
    '''
    @action(detail=False, methods=['get'])
    def reconciliation_status(self, request, tenant_id=None):
        """
        Poll reconciliation task status/result
        """
        task_id = request.query_params.get("task_id")
        if not task_id:
            return Response({"error": "task_id is required"}, status=400)

        res = AsyncResult(task_id)
        return Response({
            "task_id": task_id,
            "status": res.status,
            "result": res.result if res.ready() else None
        })
    
    
    
class UnreconciledDashboardView(APIView):
    """
    Dashboard endpoint providing aggregated metrics on unreconciled records.
    
    A record is considered unreconciled if it does not have a related Reconciliation
    with status 'matched' or 'approved'.
    
    Returns overall and daily aggregates.
    """
    def get(self, request, tenant_id=None):
        try:
            # --- BANK TRANSACTIONS ---
            # Exclude reconciled records and those with missing dates.
            bank_qs = BankTransaction.objects.exclude(
                reconciliations__status__in=['matched', 'approved']
            ).filter(date__isnull=False).exclude(date='')
            
            if tenant_id:
                bank_qs = bank_qs.filter(company__subdomain=tenant_id)
            
            bank_overall = bank_qs.aggregate(
                count=Count('id'),
                total=Coalesce(Sum('amount'), V(0, output_field=DecimalField()))
            )
            
            bank_daily = bank_qs.annotate(day=TruncDate('date')).values('day').annotate(
                count=Count('id'),
                total=Coalesce(Sum('amount'), V(0, output_field=DecimalField()))
            ).order_by('day')
            
            # --- JOURNAL ENTRIES ---
            journal_qs = JournalEntry.objects.exclude(
                reconciliations__status__in=['matched', 'approved']
            ).filter(transaction__date__isnull=False).exclude(transaction__date='')
            
            if tenant_id:
                journal_qs = journal_qs.filter(transaction__company__subdomain=tenant_id)
            
            journal_overall = journal_qs.aggregate(
                count=Count('id'),
                total=Coalesce(Sum(F('debit_amount') - F('credit_amount')), V(0, output_field=DecimalField()))
            )
            
            journal_daily = journal_qs.annotate(day=TruncDate('transaction__date')).values('day').annotate(
                count=Count('id'),
                total=Coalesce(Sum(F('debit_amount') - F('credit_amount')), V(0, output_field=DecimalField()))
            ).order_by('day')
            
            response_data = {
                "bank_transactions": {
                    "overall": bank_overall,
                    "daily": list(bank_daily)
                },
                "journal_entries": {
                    "overall": journal_overall,
                    "daily": list(journal_daily)
                }
            }
            
            return Response(response_data)
        
        except OperationalError as e:
            return Response({"error": "Database error: " + str(e)}, status=500)
        except Exception as e:
            return Response({"error": str(e)}, status=500)
    
    
class ReconciliationTaskViewSet(ScopedQuerysetMixin, viewsets.ModelViewSet):
    queryset = ReconciliationTask.objects.all().order_by("-created_at")
    serializer_class = ReconciliationTaskSerializer

    @action(detail=False, methods=["post"])
    def start(self, request, tenant_id=None):
        """
        Start reconciliation as a background task
        """
        data = request.data
        auto_match_100 = _to_bool(data.get("auto_match_100", False))
        
        # 1. Pre-create DB record with placeholder task_id
        task_obj = ReconciliationTask.objects.create(
            task_id="queued",   # will be updated after Celery fires
            tenant_id=tenant_id,
            parameters=data,
            status="queued"
        )
    
        # 2. Trigger Celery, pass the db_id
        async_result = match_many_to_many_task.delay(task_obj.id, data, tenant_id, auto_match_100)
    
        # 3. Update the DB record with Celery task_id
        task_obj.task_id = async_result.id
        task_obj.save(update_fields=["task_id"])
    
        return Response({
            "message": "Task enqueued",
            "task_id": async_result.id,   # Celery UUID
            "db_id": task_obj.id,         # persistent DB PK
        })

    @action(detail=True, methods=["get"])
    def status(self, request, pk=None, tenant_id=None):
        """
        Get status/result of a task by DB ID
        """
        task = self.get_object()
        serializer = self.get_serializer(task)
        return Response(serializer.data)
    
    @action(detail=False, methods=["get"])
    def queued(self, request, tenant_id=None):
        """
        Returns both:
        1. DB-persisted tasks (filterable by tenant_id, status)
        2. Live Celery queue info (active/reserved/scheduled)
        Filters:
          - ?tenant_id=foo
          - ?last_n=100   (last 100 tasks)
          - ?hours_ago=6  (tasks from the last 6 hours)
        """
        
        tenant_filter = request.query_params.get("tenant_id")
        status_filter = request.query_params.get("status")
        last_n = request.query_params.get("last_n")
        hours_ago = request.query_params.get("hours_ago")
        
        # ---- DB tasks ----
        qs = ReconciliationTask.objects.all().order_by("-created_at")
        if tenant_filter:
            qs = qs.filter(tenant_id=tenant_filter)
        if status_filter:
            qs = qs.filter(status=status_filter)
        
        if hours_ago:
            try:
                raw = str(hours_ago).lower()
                if raw.endswith("d"):
                    hours = int(raw[:-1]) * 24
                elif raw.endswith("h"):
                    hours = int(raw[:-1])
                else:
                    hours = int(raw)  # fallback if pure number
        
                cutoff = timezone.now() - timedelta(hours=hours)
                qs = qs.filter(created_at__gte=cutoff)
            except ValueError:
                pass
    
        if last_n:
            try:
                last_n = int(last_n)
                qs = qs.order_by("-created_at")[:last_n]
            except ValueError:
                pass
    
        
        db_tasks = self.get_serializer(qs, many=True).data

        # ---- Celery live tasks ----
        try:
            i = app.control.inspect()
            live_info = {
                "active": i.active() or {},
                "reserved": i.reserved() or {},
                "scheduled": i.scheduled() or {}
            }
        except Exception as e:
            live_info = {"error": str(e)}

        return Response({
            "db_tasks": db_tasks,
            "celery_live": live_info
        })
    
    @action(detail=False, methods=["get"])
    def task_counts(self, request, tenant_id=None):
        """
        Lightweight endpoint to return counts of tasks by status.
        Filters:
          - ?tenant_id=foo
          - ?last_n=100   (last 100 tasks)
          - ?hours_ago=6  (tasks from the last 6 hours)
        """
        tenant_filter = tenant_id
        last_n = request.query_params.get("last_n")
        hours_ago = request.query_params.get("hours_ago")
    
        qs = ReconciliationTask.objects.all()
    
        if tenant_filter:
            qs = qs.filter(tenant_id=tenant_filter)
    
        if hours_ago:
            try:
                raw = str(hours_ago).lower()
                if raw.endswith("d"):
                    hours = int(raw[:-1]) * 24
                elif raw.endswith("h"):
                    hours = int(raw[:-1])
                else:
                    hours = int(raw)  # fallback if pure number
        
                cutoff = timezone.now() - timedelta(hours=hours)
                qs = qs.filter(created_at__gte=cutoff)
            except ValueError:
                pass
    
        if last_n:
            try:
                last_n = int(last_n)
                qs = qs.order_by("-created_at")[:last_n]
            except ValueError:
                pass
    
        counts = qs.values("status").annotate(total=Count("id"))
        status_map = {row["status"]: row["total"] for row in counts}
    
        return Response({
            "running": status_map.get("running", 0),
            "completed": status_map.get("completed", 0),
            "queued": status_map.get("queued", 0),
            "failed": status_map.get("failed", 0),
        })

class ReconciliationConfigViewSet2(viewsets.ModelViewSet):
    queryset = ReconciliationConfig.objects.all()
    serializer_class = ReconciliationConfigSerializer

    def get_queryset(self):
        user = self.request.user
        company_id = self.request.query_params.get("company_id")

        qs = super().get_queryset()

        return qs.filter(
            models.Q(scope="global") |
            models.Q(scope="company", company_id=company_id) |
            models.Q(scope="user", user=user)
        )

class ReconciliationConfigViewSet(viewsets.ModelViewSet):
    queryset = ReconciliationConfig.objects.all()
    serializer_class = ReconciliationConfigSerializer

    @action(detail=False, methods=["get"])
    def resolved(self, request, *args, **kwargs):
        """
        Return all configs available to the current user:
        - Global
        - Company
        - User
        - Company+User
        """
        user = request.user
        company_id = request.query_params.get("company_id")

        qs = ReconciliationConfig.objects.filter(
            Q(scope="global")
            | Q(scope="company", company_id=company_id)
            | Q(scope="user", user=user)
            | Q(scope="company_user", company_id=company_id, user=user)
        )

        serializer = ResolvedReconciliationConfigSerializer(qs, many=True)
        return Response(serializer.data)


class EmbeddingHealthView(APIView):
    permission_classes = []  # open for quick checks

    def get(self, request, tenant_id=None):
        client = EmbeddingClient()
        t0 = time.perf_counter()
        try:
            vecs = client.embed_texts(["health check ok"])
            latency_ms = int((time.perf_counter() - t0) * 1000)
            dim = len(vecs[0]) if vecs and isinstance(vecs[0], list) else 0
            ok = (dim == settings.EMBED_DIM)
            return Response(
                {
                    "ok": ok,
                    "dim": dim,
                    "latency_ms": latency_ms,
                    "endpoint": _embed_url(),
                    "model": client.model,
                    "used_internal": bool(settings.EMBED_INTERNAL_HOST),
                },
                status=status.HTTP_200_OK if ok else status.HTTP_503_SERVICE_UNAVAILABLE,
            )
        except Exception as e:
            latency_ms = int((time.perf_counter() - t0) * 1000)
            return Response(
                {
                    "ok": False,
                    "error": str(e),
                    "latency_ms": latency_ms,
                    "endpoint": _embed_url(),
                    "model": settings.EMBED_MODEL,
                    "used_internal": bool(settings.EMBED_INTERNAL_HOST),
                },
                status=status.HTTP_503_SERVICE_UNAVAILABLE,
            )

class EmbeddingMissingCountsView(APIView):
    permission_classes = []#IsAdminUser]

    def get(self, request, tenant_id=None):
        tx  = Transaction.objects.filter(description_embedding__isnull=True).count()
        btx = BankTransaction.objects.filter(description_embedding__isnull=True).count()
        acc = Account.objects.filter(account_description_embedding__isnull=True).count()
        return Response(
            {
                "transactions_missing": tx,
                "bank_transactions_missing": btx,
                "accounts_missing": acc,
                "total_missing": tx + btx + acc,
            },
            status=status.HTTP_200_OK,
        )

class EmbeddingBackfillView(APIView):
    permission_classes = []#permissions.IsAdminUser]

    def post(self, request, tenant_id=None):
        # you can keep your serializer; here we focus on headers + optional pre-totals
        per_model_limit = int(request.data.get("per_model_limit") or 2000)
        client_opts = request.data.get("client_opts") or {}

        # enqueue with helpful headers
        headers = {
            "job_kind": "embeddings",
            "tenant_id": getattr(request.user, "company_id", None),
            "user_id": getattr(request.user, "id", None),
        }
        task = generate_missing_embeddings.apply_async(
            kwargs={"per_model_limit": per_model_limit, "client_opts": client_opts},
            headers=headers,
        )

        # (optional) prime totals right away, so UI has a target immediately
        try:
            totals = {
                "transactions": Transaction.objects.filter(description_embedding__isnull=True)[:per_model_limit].count(),
                "bank_transactions": BankTransaction.objects.filter(description_embedding__isnull=True)[:per_model_limit].count(),
                "accounts": Account.objects.filter(account_description_embedding__isnull=True)[:per_model_limit].count(),
            }
            Job.objects.filter(task_id=task.id).update(
                total=sum(totals.values()),
                by_category={"totals": totals, "done": {"transactions": 0, "bank_transactions": 0, "accounts": 0}},
            )
        except Exception:
            pass

        return Response({"task_id": task.id, "state": task.state, "mode": "async"}, status=status.HTTP_202_ACCEPTED)

STATE_MAP = {
    "PENDING":  "PENDING",
    "RECEIVED": "RECEIVED",
    "SENT":     "SENT",
    "STARTED":  "STARTED",
    "PROGRESS": "PROGRESS",   # our custom state while updating meta
    "RETRY":    "RETRY",
    "SUCCESS":  "SUCCESS",
    "FAILURE":  "FAILURE",
    "REVOKED":  "REVOKED",
}

EMBED_KIND_DEFAULT = "embeddings.backfill"

ACTIVE_STATES   = {
    STATE_MAP["PENDING"], STATE_MAP["RECEIVED"], STATE_MAP["SENT"],
    STATE_MAP["STARTED"], STATE_MAP["PROGRESS"], STATE_MAP["RETRY"],
}
FINISHED_STATES = {
    STATE_MAP["SUCCESS"], STATE_MAP["FAILURE"], STATE_MAP["REVOKED"],
}
def _friendly_state(celery_state: str) -> str:
    # Normalize to your canonical state names
    return STATE_MAP.get((celery_state or "PENDING").upper(), STATE_MAP["PENDING"])

def _parse_json_field(value: Any) -> Optional[Dict[str, Any]]:
    if value is None:
        return None
    if isinstance(value, (dict, list)):
        return value
    if isinstance(value, str) and value:
        try:
            return json.loads(value)
        except Exception:
            return {"raw": value}
    return None

def _runtime_seconds(start: Optional[datetime], end: Optional[datetime]) -> Optional[float]:
    if not start or not end:
        return None
    return (end - start).total_seconds()

def _job_to_dict(j: Job) -> Dict[str, Any]:
    return {
        "task_id": j.task_id,
        "state": j.state,                       # already one of your STATE_MAP values
        "status_friendly": j.state,
        "kind": j.kind,
        "queue": j.queue,
        "worker": j.worker,
        "created_at": j.created_at,
        "enqueued_at": j.enqueued_at,
        "started_at": j.started_at,
        "finished_at": j.finished_at,
        "runtime_s": _runtime_seconds(j.started_at or j.enqueued_at or j.created_at, j.finished_at or now()),
        "retries": j.retries,
        "max_retries": j.max_retries,
        "total": j.total,
        "done": j.done,
        "percent": j.percent,
        "by_category": j.by_category,
        "result": j.result if j.state == STATE_MAP["SUCCESS"] else None,
        "error": j.error if j.state in (STATE_MAP["FAILURE"], STATE_MAP["REVOKED"]) else None,
        "meta": j.meta,
        "tenant_id": j.tenant_id,
    }

class EmbeddingJobsListView(APIView):
    """
    GET /api/embeddings/jobs/?limit=25&status=any&kind=embeddings&include_active=1
    """
    permission_classes = []#permissions.IsAdminUser]

    def get(self, request, tenant_id=None):
        limit = int(request.query_params.get("limit", 25))
        status_filter = (request.query_params.get("status") or "any").upper()
        kind = request.query_params.get("kind") or "embeddings"
        include_active = request.query_params.get("include_active") in ("1", "true", "yes")

        qs = Job.objects.filter(kind=kind).order_by("-created_at")

        if status_filter != "ANY":
            qs = qs.filter(state=status_filter)

        finished = list(qs[:limit])
        data = {"count": len(finished), "finished": JobSerializer(finished, many=True).data}

        if include_active:
            active_states = ("PENDING", "SENT", "RECEIVED", "STARTED", "RETRY", "PROGRESS")
            active = Job.objects.filter(kind=kind, state__in=active_states).order_by("-created_at")
            data["active"] = JobSerializer(active, many=True).data
            data["active_count"] = active.count()

        return Response(data, status=status.HTTP_200_OK)


class EmbeddingTaskStatusView(APIView):
    permission_classes = []#permissions.IsAdminUser]

    def get(self, request, task_id, tenant_id=None):
        try:
            job = Job.objects.get(task_id=task_id)
        except Job.DoesNotExist:
            # still return Celery state, if any
            ar = AsyncResult(task_id)
            return Response({
                "task_id": task_id,
                "state": ar.state or "PENDING",
                "status": STATE_MAP.get(ar.state or "PENDING", "PENDING"),
                "ready": ar.ready(),
                "successful": ar.successful() if ar.ready() else False,
            }, status=status.HTTP_200_OK)

        # overlay live state from Celery (if different)
        ar = AsyncResult(task_id)
        live_state = ar.state or job.state
        info = ar.info if isinstance(ar.info, dict) else {}
        payload = JobSerializer(job).data
        payload.update({
            "state": live_state,
            "status": STATE_MAP.get(live_state, live_state),
            "ready": ar.ready(),
            "successful": ar.successful() if ar.ready() else (job.state == "SUCCESS"),
            "progress": {
                "totals": (job.by_category or {}).get("totals"),
                "done": (job.by_category or {}).get("done"),
                "remaining": (job.by_category or {}).get("remaining"),
                "done_all": (job.by_category or {}).get("done_all"),
                "remaining_all": (job.by_category or {}).get("remaining_all"),
            },
        })
        return Response(payload, status=status.HTTP_200_OK)

class EmbeddingTaskCancelView(APIView):
    permission_classes = []#permissions.IsAdminUser]

    def post(self, request, task_id, tenant_id=None):
        current_app.control.revoke(task_id, terminate=True)
        Job.objects.filter(task_id=task_id).update(
            state=STATE_MAP["REVOKED"],
            finished_at=now(),
            error="revoked",
        )
        return Response({"task_id": task_id, "revoked": True}, status=status.HTTP_200_OK)

class EmbeddingsTestView(APIView):
    """
    POST /api/embeddings/test/
    { "texts": ["hello", "banana"] }  # optional overrides also supported
    """
    permission_classes = []  # lock down if needed

    def post(self, request, tenant_id=None):
        ser = EmbedTestSerializer(data=request.data)
        ser.is_valid(raise_exception=True)
        d = ser.validated_data

        client = EmbeddingClient(
            base_url=d.get("base_url"),
            path=d.get("path"),
            model=d.get("model"),
            timeout_s=d.get("timeout_s"),
            dim=d.get("dim"),
            api_key=d.get("api_key"),
            num_thread=d.get("num_thread"),
            keep_alive=d.get("keep_alive"),
        )
        vecs = client.embed_texts(d["texts"])
        return Response(
            {"count": len(vecs), "dim": len(vecs[0]) if vecs else 0, "embeddings": vecs, "endpoint": client.url},
            status=status.HTTP_200_OK,
        )
    
def _search_qs(model, vec, field_name: str, k: int, values: list[str]):
    qs = (model.objects
          .filter(**{f"{field_name}__isnull": False})
          .annotate(score=CosineDistance(field_name, vec))
          .order_by("score")[:k])
    rows = list(qs.values(*values, "score"))
    # Attach similarity = 1 - distance (pgvector cosine distance = 1 - cosine similarity)
    for r in rows:
        dist = float(r["score"])
        r["similarity"] = 1.0 - dist
        r["score"] = dist
    return rows


class EmbeddingsSearchView(APIView):
    permission_classes = []#]IsAdminUser]

    def post(self, request, tenant_id=None):
        body = request.data or {}
        query = (body.get("query") or "").strip()
        k_each = int(body.get("k_each", 8))
        min_similarity = float(body.get("min_similarity", 0.10))
        debug_mode = body.get("debug") in (True, "true", "1") or request.query_params.get("debug") in ("1", "true", "yes")

        # Build embedding (also try an accent-less variant to diagnose)
        emb = EmbeddingClient()
        qvec = []
        qvec_noacc = []
        timings = {}
        try:
            import time
            t0 = time.perf_counter()
            qvec = emb.embed_one(query or " ")
            timings["embed"] = int((time.perf_counter() - t0) * 1000)

            if query and query != _strip_accents(query):
                qvec_noacc = emb.embed_one(_strip_accents(query))
        except Exception as e:
            log.exception("embed error")
            return Response({"ok": False, "error": str(e)}, status=502)

        # Corpus coverage
        tx_n  = Transaction.objects.filter(description_embedding__isnull=False).count()
        btx_n = BankTransaction.objects.filter(description_embedding__isnull=False).count()
        acc_n = Account.objects.filter(account_description_embedding__isnull=False).count()

        # TopK (raw)
        def _search_all(vec):
            rows = {}
            t1 = time.perf_counter()
            rows["transactions"] = _search_qs(
                Transaction, vec, "description_embedding", k_each,
                ["id", "description", "amount", "date"]
            )
            rows["bank_transactions"] = _search_qs(
                BankTransaction, vec, "description_embedding", k_each,
                ["id", "description", "amount", "date"]
            )
            rows["accounts"] = _search_qs(
                Account, vec, "account_description_embedding", k_each,
                ["id", "name", "description"]
            )
            timings["search"] = int((time.perf_counter() - t1) * 1000)
            return rows

        hits_raw = _search_all(qvec)
        # Also compute with accent-less query (for comparison) if we produced it
        hits_raw_noacc = _search_all(qvec_noacc) if qvec_noacc else None

        # Filter by min_similarity
        def _apply_threshold(rows):
            out = {}
            for bucket, items in rows.items():
                out[bucket] = [
                    {**r, "description": _snippet(r.get("description") or r.get("name"))}
                    for r in items if r["similarity"] >= min_similarity
                ]
            return out

        hits = _apply_threshold(hits_raw)
        hits_noacc = _apply_threshold(hits_raw_noacc) if hits_raw_noacc else None

        # Log key debug lines
        log.info(
            "emb.search q='%s' len=%d qstats=%s tx=%d btx=%d acc=%d min_sim=%.3f k=%d",
            _snippet(query, 120), len(qvec), _vec_stats(qvec), tx_n, btx_n, acc_n, min_similarity, k_each
        )
        if qvec_noacc:
            log.info("emb.search noacc='%s' qstats=%s", _strip_accents(query), _vec_stats(qvec_noacc))

        # Show handful of raw neighbors per bucket (even if below threshold)
        for bucket in ("transactions", "bank_transactions", "accounts"):
            sample = hits_raw[bucket][:3]
            log.debug("raw.%s: %s", bucket, [
                {
                    "id": r["id"],
                    "sim": round(r["similarity"], 4),
                    "dist": round(r["score"], 4),
                    "text": _snippet(r.get("description") or r.get("name"), 100)
                } for r in sample
            ])

        # Optional lexical sanity check (does DB contain the term?)
        lexical = None
        if debug_mode and query:
            term = query if len(query) >= 3 else None
            if term:
                lexical = {
                    "tx_like":  list(Transaction.objects.filter(description__icontains=term).values("id","description")[:3]),
                    "btx_like": list(BankTransaction.objects.filter(description__icontains=term).values("id","description")[:3]),
                    "acc_like": list(Account.objects.filter(Q(name__icontains=term) | Q(description__icontains=term)).values("id","name","description")[:3]),
                }

        # Build response
        resp = {
            "ok": True,
            "meta": {
                "query": query,
                "k_each": k_each,
                "min_similarity": min_similarity,
                "model": emb.model,
                "timings_ms": timings,
                "counts": {"transactions": tx_n, "bank_transactions": btx_n, "accounts": acc_n},
            },
            "transactions": hits["transactions"],
            "bank_transactions": hits["bank_transactions"],
            "accounts": hits["accounts"],
        }

        if debug_mode:
            resp["debug"] = {
                "query_stats": _vec_stats(qvec),
                "query_noacc_stats": _vec_stats(qvec_noacc) if qvec_noacc else None,
                "raw_top": {
                    "transactions": [
                        {"id": r["id"], "sim": r["similarity"], "dist": r["score"], "text": _snippet(r.get("description") or "", 120)}
                        for r in hits_raw["transactions"][:5]
                    ],
                    "bank_transactions": [
                        {"id": r["id"], "sim": r["similarity"], "dist": r["score"], "text": _snippet(r.get("description") or "", 120)}
                        for r in hits_raw["bank_transactions"][:5]
                    ],
                    "accounts": [
                        {"id": r["id"], "sim": r["similarity"], "dist": r["score"], "text": _snippet(r.get("name") or r.get("description") or "", 120)}
                        for r in hits_raw["accounts"][:5]
                    ],
                },
                "lexical_samples": lexical,
                "note": "similarity = 1 - cosine_distance (pgvector). If everything is < threshold, try lowering min_similarity.",
            }
            if hits_noacc is not None:
                resp["debug"]["accentless_top"] = {
                    "transactions": hits_noacc["transactions"][:5],
                    "bank_transactions": hits_noacc["bank_transactions"][:5],
                    "accounts": hits_noacc["accounts"][:5],
                }

        return Response(resp, status=status.HTTP_200_OK)